// ğŸš€ ULTIMATE PERFORMANCE OPTIMIZATION & CACHING SYSTEM
// æœ€é«˜æ€§èƒ½ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

import { logger, performance as perfTracker } from './monitoring'

// ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
const CACHE_CONFIG = {
  DEFAULT_TTL: 15 * 60 * 1000, // 15åˆ†
  MAX_CACHE_SIZE: 1000, // æœ€å¤§1000ã‚¨ãƒ³ãƒˆãƒª
  CLEANUP_INTERVAL: 5 * 60 * 1000, // 5åˆ†ã”ã¨ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  MEMORY_THRESHOLD: 0.8, // ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡80%ã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  HIT_RATE_THRESHOLD: 0.7, // ãƒ’ãƒƒãƒˆç‡70%ä»¥ä¸‹ã§æœ€é©åŒ–
}

// ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªãƒ¼
interface CacheEntry<T> {
  value: T
  timestamp: number
  ttl: number
  hitCount: number
  lastAccess: number
  size: number // ãƒã‚¤ãƒˆæ•°ï¼ˆæ¦‚ç®—ï¼‰
}

// LRU (Least Recently Used) ã‚­ãƒ£ãƒƒã‚·ãƒ¥
class LRUCache<T> {
  private cache = new Map<string, CacheEntry<T>>()
  private accessOrder: string[] = []
  private totalSize = 0
  private hitCount = 0
  private missCount = 0
  private cleanupInterval: NodeJS.Timeout
  
  constructor(
    private maxSize: number = CACHE_CONFIG.MAX_CACHE_SIZE,
    private defaultTTL: number = CACHE_CONFIG.DEFAULT_TTL
  ) {
    // å®šæœŸçš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹
    this.cleanupInterval = setInterval(() => {
      this.cleanup()
    }, CACHE_CONFIG.CLEANUP_INTERVAL)
  }
  
  set(key: string, value: T, ttl?: number): void {
    const now = Date.now()
    const entryTTL = ttl || this.defaultTTL
    const size = this.estimateSize(value)
    
    // æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤
    if (this.cache.has(key)) {
      this.delete(key)
    }
    
    // æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’ä½œæˆ
    const entry: CacheEntry<T> = {
      value,
      timestamp: now,
      ttl: entryTTL,
      hitCount: 0,
      lastAccess: now,
      size
    }
    
    this.cache.set(key, entry)
    this.accessOrder.push(key)
    this.totalSize += size
    
    // ã‚µã‚¤ã‚ºåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
    this.evictIfNecessary()
    
    logger.debug('Cache set', { key, size, ttl: entryTTL })
  }
  
  get(key: string): T | null {
    const entry = this.cache.get(key)
    const now = Date.now()
    
    if (!entry) {
      this.missCount++
      logger.debug('Cache miss', { key })
      return null
    }
    
    // TTL ãƒã‚§ãƒƒã‚¯
    if (now - entry.timestamp > entry.ttl) {
      this.delete(key)
      this.missCount++
      logger.debug('Cache expired', { key, age: now - entry.timestamp })
      return null
    }
    
    // ãƒ’ãƒƒãƒˆæ™‚ã®æ›´æ–°
    entry.hitCount++
    entry.lastAccess = now
    this.hitCount++
    
    // LRUé †åºã‚’æ›´æ–°
    this.updateAccessOrder(key)
    
    logger.debug('Cache hit', { key, hitCount: entry.hitCount })
    return entry.value
  }
  
  delete(key: string): boolean {
    const entry = this.cache.get(key)
    if (!entry) return false
    
    this.cache.delete(key)
    this.totalSize -= entry.size
    
    // ã‚¢ã‚¯ã‚»ã‚¹é †åºã‹ã‚‰ã‚‚å‰Šé™¤
    const index = this.accessOrder.indexOf(key)
    if (index > -1) {
      this.accessOrder.splice(index, 1)
    }
    
    logger.debug('Cache delete', { key })
    return true
  }
  
  has(key: string): boolean {
    const entry = this.cache.get(key)
    if (!entry) return false
    
    // TTL ãƒã‚§ãƒƒã‚¯
    const now = Date.now()
    if (now - entry.timestamp > entry.ttl) {
      this.delete(key)
      return false
    }
    
    return true
  }
  
  clear(): void {
    this.cache.clear()
    this.accessOrder = []
    this.totalSize = 0
    this.hitCount = 0
    this.missCount = 0
    logger.info('Cache cleared')
  }
  
  private updateAccessOrder(key: string): void {
    const index = this.accessOrder.indexOf(key)
    if (index > -1) {
      this.accessOrder.splice(index, 1)
    }
    this.accessOrder.push(key)
  }
  
  private evictIfNecessary(): void {
    // ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã®å‰Šé™¤
    while (this.cache.size > this.maxSize) {
      const oldestKey = this.accessOrder.shift()
      if (oldestKey) {
        this.delete(oldestKey)
      }
    }
    
    // ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ™ãƒ¼ã‚¹ã®å‰Šé™¤
    const memUsage = process.memoryUsage()
    const memoryRatio = memUsage.heapUsed / memUsage.heapTotal
    
    if (memoryRatio > CACHE_CONFIG.MEMORY_THRESHOLD) {
      const entriesToRemove = Math.floor(this.cache.size * 0.2) // 20%å‰Šé™¤
      logger.warn('High memory usage, evicting cache entries', { 
        memoryRatio, 
        entriesToRemove 
      })
      
      for (let i = 0; i < entriesToRemove && this.accessOrder.length > 0; i++) {
        const keyToRemove = this.accessOrder.shift()
        if (keyToRemove) {
          this.delete(keyToRemove)
        }
      }
    }
  }
  
  private cleanup(): void {
    const now = Date.now()
    const keysToDelete: string[] = []
    
    // æœŸé™åˆ‡ã‚Œã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’åé›†
    for (const [key, entry] of this.cache.entries()) {
      if (now - entry.timestamp > entry.ttl) {
        keysToDelete.push(key)
      }
    }
    
    // å‰Šé™¤å®Ÿè¡Œ
    keysToDelete.forEach(key => this.delete(key))
    
    if (keysToDelete.length > 0) {
      logger.info('Cache cleanup completed', { 
        expired: keysToDelete.length,
        remaining: this.cache.size
      })
    }
  }
  
  private estimateSize(value: T): number {
    try {
      // JSONæ–‡å­—åˆ—åŒ–ã—ã¦ã‚µã‚¤ã‚ºã‚’æ¦‚ç®—
      return JSON.stringify(value).length * 2 // UTF-16ãªã®ã§2å€
    } catch {
      return 1000 // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µã‚¤ã‚º
    }
  }
  
  getStats() {
    const total = this.hitCount + this.missCount
    const hitRate = total > 0 ? this.hitCount / total : 0
    
    return {
      size: this.cache.size,
      maxSize: this.maxSize,
      totalSize: this.totalSize,
      hitCount: this.hitCount,
      missCount: this.missCount,
      hitRate,
      memoryUsage: process.memoryUsage()
    }
  }
  
  destroy(): void {
    if (this.cleanupInterval) {
      clearInterval(this.cleanupInterval)
    }
    this.clear()
  }
}

// ãƒ¡ãƒ¢åŒ–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼
function memoize<T extends (...args: any[]) => any>(
  ttl: number = CACHE_CONFIG.DEFAULT_TTL,
  keyGenerator?: (...args: Parameters<T>) => string
) {
  return function (
    target: any,
    propertyKey: string,
    descriptor: PropertyDescriptor
  ) {
    const originalMethod = descriptor.value
    const cache = new LRUCache<ReturnType<T>>()
    
    descriptor.value = function (...args: Parameters<T>): ReturnType<T> {
      const key = keyGenerator ? 
        keyGenerator(...args) : 
        `${propertyKey}_${JSON.stringify(args)}`
      
      // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
      const cached = cache.get(key)
      if (cached !== null) {
        return cached
      }
      
      // ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè¡Œ
      const result = originalMethod.apply(this, args)
      
      // çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆPromiseã®å ´åˆã¯è§£æ±ºå¾Œï¼‰
      if (result instanceof Promise) {
        return result.then((resolvedResult) => {
          cache.set(key, resolvedResult, ttl)
          return resolvedResult
        })
      } else {
        cache.set(key, result, ttl)
        return result
      }
    }
    
    return descriptor
  }
}

// ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
class CacheManager {
  private static instance: CacheManager
  private caches = new Map<string, LRUCache<any>>()
  
  private constructor() {}
  
  static getInstance(): CacheManager {
    if (!CacheManager.instance) {
      CacheManager.instance = new CacheManager()
    }
    return CacheManager.instance
  }
  
  getCache<T>(name: string, maxSize?: number, defaultTTL?: number): LRUCache<T> {
    if (!this.caches.has(name)) {
      this.caches.set(name, new LRUCache<T>(maxSize, defaultTTL))
      logger.info('Cache created', { name, maxSize, defaultTTL })
    }
    return this.caches.get(name)!
  }
  
  deleteCache(name: string): boolean {
    const cache = this.caches.get(name)
    if (cache) {
      cache.destroy()
      this.caches.delete(name)
      logger.info('Cache deleted', { name })
      return true
    }
    return false
  }
  
  clearAllCaches(): void {
    for (const [name, cache] of this.caches.entries()) {
      cache.clear()
      logger.info('Cache cleared', { name })
    }
  }
  
  getAllStats() {
    const stats: Record<string, any> = {}
    for (const [name, cache] of this.caches.entries()) {
      stats[name] = cache.getStats()
    }
    return stats
  }
  
  optimizeAll(): void {
    for (const [name, cache] of this.caches.entries()) {
      const stats = cache.getStats()
      
      // ãƒ’ãƒƒãƒˆç‡ãŒä½ã„å ´åˆã¯è­¦å‘Š
      if (stats.hitRate < CACHE_CONFIG.HIT_RATE_THRESHOLD) {
        logger.warn('Low cache hit rate detected', { name, hitRate: stats.hitRate })
      }
      
      // ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã‚‹å ´åˆã¯ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
      if (stats.size > stats.maxSize * 0.9) {
        logger.info('Cache approaching size limit, triggering cleanup', { 
          name, 
          size: stats.size, 
          maxSize: stats.maxSize 
        })
      }
    }
  }
}

// éåŒæœŸé–¢æ•°ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
class AsyncCache<T> {
  private cache = new LRUCache<Promise<T>>()
  private pending = new Map<string, Promise<T>>()
  
  async get(
    key: string,
    factory: () => Promise<T>,
    ttl?: number
  ): Promise<T> {
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
    const cached = this.cache.get(key)
    if (cached) {
      try {
        return await cached
      } catch (error) {
        // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå¤±æ•—ã—ãŸPromiseã‚’å‰Šé™¤
        this.cache.delete(key)
      }
    }
    
    // é€²è¡Œä¸­ã®å‡¦ç†ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    const pendingPromise = this.pending.get(key)
    if (pendingPromise) {
      return await pendingPromise
    }
    
    // æ–°ã—ã„å‡¦ç†ã‚’é–‹å§‹
    const promise = factory().finally(() => {
      this.pending.delete(key)
    })
    
    this.pending.set(key, promise)
    this.cache.set(key, promise, ttl)
    
    return await promise
  }
  
  delete(key: string): void {
    this.cache.delete(key)
    this.pending.delete(key)
  }
  
  clear(): void {
    this.cache.clear()
    this.pending.clear()
  }
}

// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
class PerformanceOptimizer {
  private static instance: PerformanceOptimizer
  private batchQueues = new Map<string, { items: any[]; timeout: NodeJS.Timeout }>()
  
  private constructor() {}
  
  static getInstance(): PerformanceOptimizer {
    if (!PerformanceOptimizer.instance) {
      PerformanceOptimizer.instance = new PerformanceOptimizer()
    }
    return PerformanceOptimizer.instance
  }
  
  // ãƒãƒƒãƒå‡¦ç†
  batch<T, R>(
    key: string,
    item: T,
    processor: (items: T[]) => Promise<R[]>,
    delay: number = 100,
    maxBatchSize: number = 10
  ): Promise<R> {
    return new Promise((resolve, reject) => {
      let queue = this.batchQueues.get(key)
      
      if (!queue) {
        queue = { items: [], timeout: setTimeout(() => {}, 0) }
        this.batchQueues.set(key, queue)
      }
      
      queue.items.push({ item, resolve, reject })
      
      // ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸå ´åˆã¯å³åº§ã«å‡¦ç†
      if (queue.items.length >= maxBatchSize) {
        this.processBatch(key, processor)
      } else {
        // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        clearTimeout(queue.timeout)
        queue.timeout = setTimeout(() => {
          this.processBatch(key, processor)
        }, delay)
      }
    })
  }
  
  private async processBatch<T, R>(
    key: string,
    processor: (items: T[]) => Promise<R[]>
  ): Promise<void> {
    const queue = this.batchQueues.get(key)
    if (!queue || queue.items.length === 0) return
    
    const items = queue.items.splice(0)
    this.batchQueues.delete(key)
    
    try {
      const inputItems = items.map(({ item }) => item)
      const results = await processor(inputItems)
      
      items.forEach(({ resolve }, index) => {
        resolve(results[index])
      })
    } catch (error) {
      items.forEach(({ reject }) => {
        reject(error)
      })
    }
  }
  
  // ãƒ‡ãƒã‚¦ãƒ³ã‚¹
  debounce<T extends (...args: any[]) => any>(
    func: T,
    delay: number
  ): (...args: Parameters<T>) => void {
    let timeoutId: NodeJS.Timeout
    
    return (...args: Parameters<T>) => {
      clearTimeout(timeoutId)
      timeoutId = setTimeout(() => func(...args), delay)
    }
  }
  
  // ã‚¹ãƒ­ãƒƒãƒˆãƒ«
  throttle<T extends (...args: any[]) => any>(
    func: T,
    delay: number
  ): (...args: Parameters<T>) => void {
    let lastExecTime = 0
    
    return (...args: Parameters<T>) => {
      const now = Date.now()
      if (now - lastExecTime >= delay) {
        func(...args)
        lastExecTime = now
      }
    }
  }
}

// ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
export const cacheManager = CacheManager.getInstance()
export const performanceOptimizer = PerformanceOptimizer.getInstance()
export { LRUCache, AsyncCache, memoize }

// ä¾¿åˆ©ãªãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
export function withCache<T>(
  cacheKey: string,
  factory: () => Promise<T>,
  ttl?: number
): Promise<T> {
  const cache = cacheManager.getCache<T>('default')
  const asyncCache = new AsyncCache<T>()
  
  return asyncCache.get(cacheKey, factory, ttl)
}

export function cached<T extends (...args: any[]) => any>(
  ttl?: number,
  keyGenerator?: (...args: Parameters<T>) => string
) {
  return memoize<T>(ttl, keyGenerator)
}

// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šä»˜ãã‚­ãƒ£ãƒƒã‚·ãƒ¥
export async function withPerformanceCache<T>(
  cacheKey: string,
  factory: () => Promise<T>,
  ttl?: number
): Promise<T> {
  const trackingId = `cache_${cacheKey}`
  
  return await perfTracker.measure(trackingId, async () => {
    return await withCache(cacheKey, factory, ttl)
  }).then(({ result, duration }) => {
    logger.debug('Cache operation completed', {
      cacheKey,
      duration,
      cached: duration < 10 // 10msæœªæº€ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã¨æ¨å®š
    })
    return result
  })
}